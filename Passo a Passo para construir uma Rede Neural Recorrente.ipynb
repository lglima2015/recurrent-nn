{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Construindo sua Rede Neural Recorrente - Passo a Passo - Tradução Coursera - Recurrent Neural Networks\n",
    "As Redes Neurais Recorrentes (RNN) são muito eficiente para o Processamento de Linguagem Natural e outras tarefas sequenciais porque de uma certa forma elas possuem \"memória\". Eles conseguem ler inputs $x^{\\langle t \\rangle}$ (como as palavras) um a um, e lembrar de alguma informação/contexto através de ativações dentro das camadas ocultas da rede que são passadas de uma por vez para a próxima. Isso permite que uma RNN unidirecional pegar uma informação do passado para processar um input a frente. \n",
    "\n",
    "**Notação**:\n",
    "\n",
    "- Superscript $[l]$ significa um objeto associado com a  $l^{th}$ camada. \n",
    "    - Exemplo: $a^{[4]}$ é a $4^{th}$ ativação da camada. $W^{[5]}$ e $b^{[5]}$ são os $5^{th}$ parâmetros da camada.\n",
    "\n",
    "- Superscript $(i)$ significa um objeto associado com o  $i^{th}$ exemplo. \n",
    "    - Exemplo: $x^{(i)}$ é o $i^{th}$ input exemplo de treinamentoi.\n",
    "\n",
    "- Superscript $\\langle t \\rangle$ significa o objeto no tempo $t^{th}$ . \n",
    "    - Exemplo: $x^{\\langle t \\rangle}$ é o input x no tempo $t^{th}$ . $x^{(i)\\langle t \\rangle}$ é o input no tempo $t^{th}$ do exemplo $i$.\n",
    "    \n",
    "- Lowerscript $i$ significa a $i^{th}$ entrada do vetor.\n",
    "    - Exemplo: $a^{[l]}_i$ significa a $i^{th}$ entrada da ativação na camada $l$.\n",
    "\n",
    "Estamos assumindo que o leitor já tenha alguma familiaridade com `numpy` e/ou fez algum curso sobre o assunto. Então, vamos começar!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "O primeito passo é importar todos os pacotes que você ira utilizar neste tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rnn_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1 - Propoagação para frente para Redes Neurais Recorrentes básicas\n",
    "\n",
    "Nos próximo tutorias você será capaz de gerar música utilizando RNN. A RNN básica que você irá implementar tem a estrutura abaixo. Neste exemplo, $T_x = T_y$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"images/RNN.png\" style=\"width:500;height:300px;\">\n",
    "<caption><center> **Figure 1**: Basic RNN model </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Veja os passos para implementar uma RNN: \n",
    "\n",
    "**Passos**:\n",
    "1. Implementar os cálculos necessários para computar um passo no tempo da RNN.\n",
    "2. Implementar o loop sobre os passos $T_x$ para processar na sequencia todas as entradas, uma por vez. \n",
    "\n",
    "Vamos lá!\n",
    "\n",
    "## 1.1 - Célula RNN\n",
    "\n",
    "Uma Rede Neural Recorrente pode ser entendida com a repetição de uma célula mais simples. A Ideia é você primeiro implementar a computação necessária para um único passo no tempo. A figura abaixo descreve as operações de uma célula RNN em um único passo de tempo.\n",
    "\n",
    "<img src=\"images/rnn_step_forward.png\" style=\"width:700px;height:300px;\">\n",
    "<caption><center> **Figura 2**: Uma célula RNN básica. Pega uma entrada (input) $x^{\\langle t \\rangle}$ (atual input) e $a^{\\langle t - 1\\rangle}$ (estado anterior escondido contendo informação do passadp), e as saídas (outputs) $a^{\\langle t \\rangle}$ que é levada para a próxima célula RNN e também utilizada para prever $y^{\\langle t \\rangle}$ </center></caption>\n",
    "\n",
    "**Exercício**: Implementar a Célula-RNN descrita na figura (2).\n",
    "\n",
    "**Instruções**:\n",
    "1. Computar o estado escondido com a função de ativação tanh (tangente hiperbólica): $a^{\\langle t \\rangle} = \\tanh(W_{aa} a^{\\langle t-1 \\rangle} + W_{ax} x^{\\langle t \\rangle} + b_a)$.\n",
    "2. Utiliza seu novo estado escondido $a^{\\langle t \\rangle}$, para computar a previsão $\\hat{y}^{\\langle t \\rangle} = softmax(W_{ya} a^{\\langle t \\rangle} + b_y)$. Nós forneceremos a você a função: `softmax`.\n",
    "3. Armazene $(a^{\\langle t \\rangle}, a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}, parameters)$ em cache\n",
    "4. Retorne $a^{\\langle t \\rangle}$ , $y^{\\langle t \\rangle}$ e o cache\n",
    "\n",
    "Nós iremos vetorizar os $m$ exemplos. Então, $x^{\\langle t \\rangle}$ terá a dimensão $(n_x,m)$, e $a^{\\langle t \\rangle}$ terá a dimensão $(n_a,m)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# FUNÇÃO CLASSIFICADA: rnn_cell_forward\n",
    "\n",
    "def rnn_cell_forward(xt, a_prev, parameters):\n",
    "    \"\"\"\n",
    "    Implementa uma único passo pra frente da célual-RNN como descrito na Figura (2)\n",
    "\n",
    "    Argumentos:\n",
    "    xt -- seu dado de entrada no tempo \"t\", um vetor numpy de dimensões (n_x, m).\n",
    "    a_prev -- Estado oculto no tempo \"t-1\", um vetor numpy de dimensões (n_a, m)\n",
    "    parameters -- dicionário python contendo:\n",
    "                        Wax -- Matriz de Pesos que multipliaca a entrada (input), vetor numpy de dimensões (n_a, n_x)\n",
    "                        Waa -- Matriz de Pesos que multipliaca o estado escondido, vetor numpy de dimensões (n_a, n_a)\n",
    "                        Wya -- Matriz de Pesos que relaciona o estado escondido com a saída (output), vetor numpy de dimensões (n_y, n_a)\n",
    "                        ba --  Bias, vetor numpy de dimensões (n_a, 1)\n",
    "                        by -- Bias que relacion o estado escondido com a saída (output), vetor numpy de dimensões (n_y, 1)\n",
    "    Retorno:\n",
    "    a_next -- o próximo estado escondido da célula, de dimensões (n_a, m)\n",
    "    yt_pred -- Previsão no tempo \"t\", vetor numpy de dimensões (n_y, m)\n",
    "    cache -- Tupla de valores necessários para o passo para trás, contendo (a_next, a_prev, xt, parameters)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Recupere os parâmetros de \"parameters\"\n",
    "    Wax = parameters[\"Wax\"]\n",
    "    Waa = parameters[\"Waa\"]\n",
    "    Wya = parameters[\"Wya\"]\n",
    "    ba = parameters[\"ba\"]\n",
    "    by = parameters[\"by\"]\n",
    "    \n",
    "    ### INCIE O SEU CÓDIGO AQUI ### (≈2 lines)\n",
    "    # Compute a próximo estado de ativação usando a fórmula dada acima\n",
    "    a_next = np.tanh(np.dot(Wax, xt) + np.dot(Waa, a_prev) + ba)\n",
    "    # Compute a saída da célula atual usando a fórmula dada acima\n",
    "    yt_pred = softmax(np.dot(Wya, a_next) + by)  \n",
    "    ### SEU CÓDIGO FINALIZA AQUI ###\n",
    "    \n",
    "    # Guarder os valores necessários para a propagação para trás na variável cache\n",
    "    cache = (a_next, a_prev, xt, parameters)\n",
    "    \n",
    "    return a_next, yt_pred, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_next[4] =  [ 0.59584544  0.18141802  0.61311866  0.99808218  0.85016201  0.99980978\n",
      " -0.18887155  0.99815551  0.6531151   0.82872037]\n",
      "a_next.shape =  (5, 10)\n",
      "yt_pred[1] = [ 0.9888161   0.01682021  0.21140899  0.36817467  0.98988387  0.88945212\n",
      "  0.36920224  0.9966312   0.9982559   0.17746526]\n",
      "yt_pred.shape =  (2, 10)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "xt = np.random.randn(3,10)\n",
    "a_prev = np.random.randn(5,10)\n",
    "Waa = np.random.randn(5,5)\n",
    "Wax = np.random.randn(5,3)\n",
    "Wya = np.random.randn(2,5)\n",
    "ba = np.random.randn(5,1)\n",
    "by = np.random.randn(2,1)\n",
    "parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    "\n",
    "a_next, yt_pred, cache = rnn_cell_forward(xt, a_prev, parameters)\n",
    "print(\"a_next[4] = \", a_next[4])\n",
    "print(\"a_next.shape = \", a_next.shape)\n",
    "print(\"yt_pred[1] =\", yt_pred[1])\n",
    "print(\"yt_pred.shape = \", yt_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Saída Esperada**: \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **a_next[4]**:\n",
    "        </td>\n",
    "        <td>\n",
    "           [ 0.59584544  0.18141802  0.61311866  0.99808218  0.85016201  0.99980978\n",
    " -0.18887155  0.99815551  0.6531151   0.82872037]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **a_next.shape**:\n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 10)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **yt[1]**:\n",
    "        </td>\n",
    "        <td>\n",
    "           [ 0.9888161   0.01682021  0.21140899  0.36817467  0.98988387  0.88945212\n",
    "  0.36920224  0.9966312   0.9982559   0.17746526]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **yt.shape**:\n",
    "        </td>\n",
    "        <td>\n",
    "           (2, 10)\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1.2 - Passo a frente RNN \n",
    "\n",
    "Podemos ver uma RNN como a repetição da célula que você acabou de construir. Se os dados da sequência de entrada são carregados por 10 passos de tempo, então você copiará a sua célula RNN, 10 veses. Cada célula pega como entrada o estado escondido da célula anterior ($a^{\\langle t-1 \\rangle}$) e o dado de entrada no tempo t atual ($x^{\\langle t \\rangle}$). Sua saída é o estado oculto ($a^{\\langle t \\rangle}$) e a previsão ($y^{\\langle t \\rangle}$) para o passo de tempo atual.\n",
    "\n",
    "\n",
    "<img src=\"images/rnn2.png\" style=\"width:800px;height:300px;\">\n",
    "<caption><center> **Figure 3**: RNN Básica. A sequencia de entrada $x = (x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, ..., x^{\\langle T_x \\rangle})$  é propagada em $T_x$ passos de tempo. A saída da rede $y = (y^{\\langle 1 \\rangle}, y^{\\langle 2 \\rangle}, ..., y^{\\langle T_x \\rangle})$. </center></caption>\n",
    "\n",
    "\n",
    "\n",
    "**Exercício**: Código para a propagação da RNN descrita na Figura (3).\n",
    "\n",
    "**Instruções**:\n",
    "1. Cria um vetor de zeros ($a$) que armazenará todos os estados ocultos da RNN.\n",
    "2. Inicialize o \"next\" estado escondido como $a_0$ (estado escondido inicial).\n",
    "3. Itere sobre cada passo de tempo, com seu indice incremental de $t$ :\n",
    "    - Atualize o \"next\" estado oculto e o cache executando `rnn_cell_forward`\n",
    "    - Armazene o \"next\" estado oculto em  $a$ na posição ($t^{th}$) \n",
    "    - Armazene a previsão em y\n",
    "    - Adicione o cache para a lista de caches\n",
    "4. Retorne $a$, $y$ e os caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: rnn_forward\n",
    "\n",
    "def rnn_forward(x, a0, parameters):\n",
    "    \"\"\"\n",
    "    Implementar a propagação para frente da rede neural recorrente descrita na Figura (3).\n",
    "\n",
    "    Argumentos:\n",
    "    x -- dado de entrada para cada passo de tempo , de dimensões (n_x, m, T_x).\n",
    "    a0 -- estado incial oculto, de dimensões (n_a, m)\n",
    "    parameters -- dicionário python contendo:\n",
    "                    Wax -- Matriz de Pesos que multipliaca a entrada (input), vetor numpy de dimensões (n_a, n_x)\n",
    "                    Waa -- Matriz de Pesos que multipliaca o estado escondido, vetor numpy de dimensões (n_a, n_a)\n",
    "                    Wya -- Matriz de Pesos que relaciona o estado escondido com a saída (output), vetor numpy de dimensões (n_y, n_a)\n",
    "                    ba --  Bias, vetor numpy de dimensões (n_a, 1)\n",
    "                    by -- Bias que relacion o estado escondido com a saída (output), vetor numpy de dimensões (n_y, 1)\n",
    "\n",
    "    Retornos:\n",
    "    a -- Estados escondidos para cada passo de tempo, array numpy de dimensões (n_a, m, T_x)\n",
    "    y_pred -- Previsões para cada passo de tempo, array numpy de dimensões (n_y, m, T_x)\n",
    "    caches -- Tuplas de valores necessários para o passo pra trás, contendo (list of caches, x)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Inicializa \"caches\" que conterá a lista de todos os caches\n",
    "    caches = []\n",
    "    \n",
    "    # Recupe as dimensões do formato de x e  parameters[\"Wya\"]\n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_a = parameters[\"Wya\"].shape\n",
    "    \n",
    "    ### INICIE SEU CÓDIGO AQUI ###\n",
    "    \n",
    "    # inicialize \"a\" e \"y\" com zeros (≈2 lines)\n",
    "    a = np.zeros((n_a, m, T_x), dtype=float)\n",
    "    y_pred = np.zeros((n_y, m, T_x), dtype=float)\n",
    "    \n",
    "    # Inicialize a_next (≈1 linha)\n",
    "    a_next = a0\n",
    "    # Itere sobre todos os passos de tempo\n",
    "    for t in range(T_x):\n",
    "        # Atualize o próximo estado oculto, compute a previsão, pegue o cache (≈1 linha)\n",
    "        a_next, yt_pred, cache = rnn_cell_forward(x[:,:,t], a_next, parameters)\n",
    "        # Salve o valor do novo estado oculto \"next\" em a (≈1 linha)\n",
    "        a[:,:,t] = a_next\n",
    "        # Salve o valor da previsão em y (≈1 linha)\n",
    "        y_pred[:,:,t] = yt_pred\n",
    "        # Concatene (append) \"cache\" em \"caches\" (≈1 linha)\n",
    "        caches.append(cache)\n",
    "        \n",
    "    ### FIM DO SEU CÓDIGO ###\n",
    "    \n",
    "    # Aramzene valores necessários para propagação para trás no cache\n",
    "    caches = (caches, x)\n",
    "    \n",
    "    return a, y_pred, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a[4][1] =  [-0.99999375  0.77911235 -0.99861469 -0.99833267]\n",
      "a.shape =  (5, 10, 4)\n",
      "y_pred[1][3] = [ 0.79560373  0.86224861  0.11118257  0.81515947]\n",
      "y_pred.shape =  (2, 10, 4)\n",
      "caches[1][1][3] = [-1.1425182  -0.34934272 -0.20889423  0.58662319]\n",
      "len(caches) =  2\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(3,10,4)\n",
    "a0 = np.random.randn(5,10)\n",
    "Waa = np.random.randn(5,5)\n",
    "Wax = np.random.randn(5,3)\n",
    "Wya = np.random.randn(2,5)\n",
    "ba = np.random.randn(5,1)\n",
    "by = np.random.randn(2,1)\n",
    "parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    "\n",
    "a, y_pred, caches = rnn_forward(x, a0, parameters)\n",
    "print(\"a[4][1] = \", a[4][1])\n",
    "print(\"a.shape = \", a.shape)\n",
    "print(\"y_pred[1][3] =\", y_pred[1][3])\n",
    "print(\"y_pred.shape = \", y_pred.shape)\n",
    "print(\"caches[1][1][3] =\", caches[1][1][3])\n",
    "print(\"len(caches) = \", len(caches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Saída Esperada**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **a[4][1]**:\n",
    "        </td>\n",
    "        <td>\n",
    "           [-0.99999375  0.77911235 -0.99861469 -0.99833267]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **a.shape**:\n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 10, 4)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **y[1][3]**:\n",
    "        </td>\n",
    "        <td>\n",
    "           [ 0.79560373  0.86224861  0.11118257  0.81515947]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **y.shape**:\n",
    "        </td>\n",
    "        <td>\n",
    "           (2, 10, 4)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **cache[1][1][3]**:\n",
    "        </td>\n",
    "        <td>\n",
    "           [-1.1425182  -0.34934272 -0.20889423  0.58662319]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **len(cache)**:\n",
    "        </td>\n",
    "        <td>\n",
    "           2\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parabéns! Você consegui construir um propagação para frente de uma rede neural recorrente do zero. Este trabalho servirá para várias aplicações, Mas sofre do famoso problema do gradiente que desaparece (vanishing gradient problems). Então, ele funciona melhor quando cada saída $y^{\\langle t \\rangle}$ pode ser estimada usando principalmente contexto \"local\"  (significa que a informações das entradas $x^{\\langle t' \\rangle}$ onde  $t'$ não é tão distante de $t$). \n",
    "\n",
    "Na próxima estapa, você irá desenvolver um modelo mais complexo chamado LSTM, que endereça melhor o problema do gradiente que desaparece. O modelo LSTM será mais capaz de lembrar a peça de informação mantida salva por vários passos de tempo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
